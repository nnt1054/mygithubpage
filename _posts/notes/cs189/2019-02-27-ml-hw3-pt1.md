---
layout: post
title:  "Decision Theory"
date:   2019-02-27 00:00:00 -0800
categories: notes
tags: cs189
---

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

#### Decision Theory (Risk Minimization)
For a given feature, multiple sample points with the same value can belong to different classes.  For example, among people that eat 1200 Calories a day, there can exist people that have cancer and people that don't have cancer.  
Recall that:
>P(X) = P(X\|Y = 1) P(Y = 1) + P(X\|Y = 0) P(Y = 0)

We use Bayes theorem to determine the probability of an event given information:  
>P(Y = 1\|X) = P(X\|Y = 1)P(Y = 1) / P(X)

A loss function **L(x,y)** specifies the magnitude of a penalty if classifier predicts z, where the true class is y.  For example, a function **L(x,y)** could output 1 for false positives, 5 for false negatvies, and 0 for when **z == y**.  A loss function is **asymmetrical** when not all false classifications aren't weighed the same.  A **0 - 1 loss function** simply outputs 1 for all incorrect positions, 0 for correct.  


-bayes decision rule is using risk to pick the optimal classifier
-also 'bayes classifier': r* that minimizes function R(r)


#### Gaussian Function:
A Gaussian is a function of the form:    $$f(x) = ae^{ {x - b}^2 \over {2c^2} }$$
where **a** is the height of the peak, **b** the position, and **c** the width of the curve.