---
layout: post
title:  "Machine Learning HW01 Part 1"
date:   2019-02-01 19:46:40 -0800
categories: notes
tags: cs189
---

#### Data Paritioning:
* it's common to have to partition available labeled data into your own "training" and "validation" data sets
* for samller data sets, you can use `K-Fold Cross-Validation`, where you partition the training data into k disjoint sets, and then train a classifier k times using each of the k sets for validation, and the other k-1 sets for training.
* to shuffle a data set, you can create a random permutation of indices, then rearrange the data set using the random permutation.

{% highlight python %}
dataFile = io.loadmat("data_file.mat")
dataLength = len(dataFile["training_data"])
rand_idx = np.random.permutation(np.arange(dataLength))
train, labels = dataFile["training_data"][rand_idx], dataFile["training_labels"][rand_idx]
{% endhighlight %}



don't use train_test_split btw
no way to partially load in a .mat file, can choose which variable you want though
np.random.shuffle vs np.random.permutation


images represented as a row vector of pixel brightness
`Classification Accuracy` to measure error rate
train a `linear support vector machine` or SVM
-use sklearn for the SVM model and accuracy metric function

vary number of training examples from mnist (use indices after 10000)
train over integers(0-255) or floats(0-1) for consistency

1. create sklearn.svm model
can use `SVC(kernel=’linear’)` or `LinearSVC` for training
train new SVM instances for each training set size btw

LinearSVC(X, Y) where
X: training data array w/ size [num_samples, n_features]
Y: array of class labels w/ size [num_samples]
-the samples at X[index] belongs to the class Y[index]

getting: `Liblinear failed to converge, increase the number of iterations.`, meaning I should try some preprocessing before the training

Data Preprocessing:  
- rescaling data
- convert to binary
- standardizing
- scaling/decomposition/aggregation


spam thing:
each sample is a document
each sample has a vector where each entry is how many times ith word appears in the documents
spam_labels is whether or not a document is spam

Hyperparameter Tuning:
`hyperparamters` you can tune to influence the parameters
ex: regularization parameter C in soft-margin SVM alg? (lol what)
ok so its like  
every kinda ML algorithm has one or more `hyperparamters`, and theyre different for each algorithm.  For example, k-nearest's hyperparamter would be the number of k neighbors.  
So right now, I gotta figure out what ML algorithm I'm using rn and what it's hyperparameter it is.  Apparently I'm using 'soft-margin SVM', and the paramter I'm working with is the 'regularization parameter C'

im heckin blanking out on how to fit a line on some data points


K-Fold Cross-Validation:  
-useful for smaller datasets
-split data into K disjoint (no shared elements) sets
-for each of the k sets, train on the other k-1 sets and train with kth set

too heckin slow for bigger numbers
14 was the highest using the full data set
25 was the highest using only have the data set
	-probably under fitting the data though?

i can write more thoughts and observations later