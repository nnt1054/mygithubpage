<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Neil Toledo - Machine Learning HW01 Part 1</title>
  <meta name="description" content="Data Paritioning:  it’s common to have to partition available labeled data into your own “training” and “validation” data sets  for samller data sets, you ca...">

  <link rel="stylesheet" href="/assets/css/rouge.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link href='https://fonts.googleapis.com/css?family=Comfortaa' rel='stylesheet'>
</head>

  <body>
    <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Machine Learning HW01 Part 1</h1>
    <p class="post-meta"><time datetime="2019-02-01T19:46:40-08:00" itemprop="datePublished">Feb 1, 2019</time></p>
  </header>

  <link rel="stylesheet" href="/assets/css/posts.css">
  <a id="back-id" class="rect-btn center-container height32 back-btn" href=/blog/notes> <span>&#8592Back</span> </a>

  <div class="post-content" itemprop="articleBody">
    <h4 id="data-paritioning">Data Paritioning:</h4>
<ul>
  <li>it’s common to have to partition available labeled data into your own “training” and “validation” data sets</li>
  <li>for samller data sets, you can use <code class="highlighter-rouge">K-Fold Cross-Validation</code>, where you partition the training data into k disjoint sets, and then train a classifier k times using each of the k sets for validation, and the other k-1 sets for training.</li>
  <li>to shuffle a data set, you can create a random permutation of indices, then rearrange the data set using the random permutation.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">dataFile</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">loadmat</span><span class="p">(</span><span class="s">"data_file.mat"</span><span class="p">)</span>
<span class="n">dataLength</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataFile</span><span class="p">[</span><span class="s">"training_data"</span><span class="p">])</span>
<span class="n">rand_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">dataLength</span><span class="p">))</span>
<span class="n">train</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataFile</span><span class="p">[</span><span class="s">"training_data"</span><span class="p">][</span><span class="n">rand_idx</span><span class="p">],</span> <span class="n">dataFile</span><span class="p">[</span><span class="s">"training_labels"</span><span class="p">][</span><span class="n">rand_idx</span><span class="p">]</span></code></pre></figure>

<p>don’t use train_test_split btw
no way to partially load in a .mat file, can choose which variable you want though
np.random.shuffle vs np.random.permutation</p>

<p>images represented as a row vector of pixel brightness
<code class="highlighter-rouge">Classification Accuracy</code> to measure error rate
train a <code class="highlighter-rouge">linear support vector machine</code> or SVM
-use sklearn for the SVM model and accuracy metric function</p>

<p>vary number of training examples from mnist (use indices after 10000)
train over integers(0-255) or floats(0-1) for consistency</p>

<ol>
  <li>create sklearn.svm model
can use <code class="highlighter-rouge">SVC(kernel=’linear’)</code> or <code class="highlighter-rouge">LinearSVC</code> for training
train new SVM instances for each training set size btw</li>
</ol>

<p>LinearSVC(X, Y) where
X: training data array w/ size [num_samples, n_features]
Y: array of class labels w/ size [num_samples]
-the samples at X[index] belongs to the class Y[index]</p>

<p>getting: <code class="highlighter-rouge">Liblinear failed to converge, increase the number of iterations.</code>, meaning I should try some preprocessing before the training</p>

<p>Data Preprocessing:</p>
<ul>
  <li>rescaling data</li>
  <li>convert to binary</li>
  <li>standardizing</li>
  <li>scaling/decomposition/aggregation</li>
</ul>

<p>spam thing:
each sample is a document
each sample has a vector where each entry is how many times ith word appears in the documents
spam_labels is whether or not a document is spam</p>

<p>Hyperparameter Tuning:
<code class="highlighter-rouge">hyperparamters</code> you can tune to influence the parameters
ex: regularization parameter C in soft-margin SVM alg? (lol what)
ok so its like<br />
every kinda ML algorithm has one or more <code class="highlighter-rouge">hyperparamters</code>, and theyre different for each algorithm.  For example, k-nearest’s hyperparamter would be the number of k neighbors.<br />
So right now, I gotta figure out what ML algorithm I’m using rn and what it’s hyperparameter it is.  Apparently I’m using ‘soft-margin SVM’, and the paramter I’m working with is the ‘regularization parameter C’</p>

<p>im heckin blanking out on how to fit a line on some data points</p>

<p>K-Fold Cross-Validation:<br />
-useful for smaller datasets
-split data into K disjoint (no shared elements) sets
-for each of the k sets, train on the other k-1 sets and train with kth set</p>

<p>too heckin slow for bigger numbers
14 was the highest using the full data set
25 was the highest using only have the data set
	-probably under fitting the data though?</p>

<p>i can write more thoughts and observations later</p>

  </div>

</article>
  </body>


  <script src="/assets/js/jquery-3.3.1.min.js"></script>
  <script src="/assets/js/main.js"></script>
  <link rel="stylesheet" href="/assets/css/mobile.css">
</html>